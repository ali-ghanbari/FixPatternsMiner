diff --git a/cdap-data-fabric-tests/src/test/java/co/cask/cdap/data/stream/StreamInputFormatTest.java b/cdap-data-fabric-tests/src/test/java/co/cask/cdap/data/stream/StreamInputFormatTest.java
index 55d26d3..77b6566 100644
--- a/cdap-data-fabric-tests/src/test/java/co/cask/cdap/data/stream/StreamInputFormatTest.java
+++ b/cdap-data-fabric-tests/src/test/java/co/cask/cdap/data/stream/StreamInputFormatTest.java
@@ -17,0 +18 @@
+import co.cask.cdap.api.common.Bytes;
@@ -30,0 +32,5 @@
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.JobContextImpl;
+import org.apache.hadoop.mapred.JobID;
+import org.apache.hadoop.mapred.TaskAttemptID;
+import org.apache.hadoop.mapreduce.InputSplit;
@@ -33,0 +40 @@
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
@@ -34,0 +42 @@
+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
@@ -43,0 +52 @@
+import java.util.List;
@@ -242,0 +252,41 @@
+  @Test
+  public void testStreamRecordReader() throws Exception {
+    File inputDir = tmpFolder.newFolder();
+    File partition = new File(inputDir,  "1.1000");
+    partition.mkdirs();
+    File eventFile = new File(partition, "bucket.1.0." + StreamFileType.EVENT.getSuffix());
+    File indexFile = new File(partition, "bucket.1.0." + StreamFileType.INDEX.getSuffix());
+
+    // write 1 event
+    StreamDataFileWriter writer = new StreamDataFileWriter(Files.newOutputStreamSupplier(eventFile),
+                                                           Files.newOutputStreamSupplier(indexFile),
+                                                           100L);
+    writer.append(StreamFileTestUtils.createEvent(1000, "test"));
+
+    // get splits from the input format. Expect to get 2 splits,
+    // one from 0 - some offset and one from offset - Long.MAX_VALUE.
+    Configuration conf = new Configuration();
+    TaskAttemptContext context = new TaskAttemptContextImpl(conf, new TaskAttemptID());
+    StreamInputFormat.setStreamPath(conf, inputDir.toURI());
+    StreamInputFormat format = new StreamInputFormat();
+    List<InputSplit> splits = format.getSplits(new JobContextImpl(new JobConf(conf), new JobID()));
+    Assert.assertEquals(2, splits.size());
+
+    // write another event so that the 2nd split has something to read
+    writer.append(StreamFileTestUtils.createEvent(1001, "test"));
+    writer.close();
+
+    // create a record reader for the 2nd split
+    StreamRecordReader<LongWritable, StreamEvent> recordReader =
+      new StreamRecordReader<LongWritable, StreamEvent>(new IdentityStreamEventDecoder());
+    recordReader.initialize(splits.get(1), context);
+
+    // check that we read the 2nd stream event
+    Assert.assertTrue(recordReader.nextKeyValue());
+    StreamEvent output = recordReader.getCurrentValue();
+    Assert.assertEquals(1001, output.getTimestamp());
+    Assert.assertEquals("test", Bytes.toString(output.getBody()));
+    // check that there is nothing more to read
+    Assert.assertFalse(recordReader.nextKeyValue());
+  }
+
diff --git a/cdap-data-fabric/src/main/java/co/cask/cdap/data/stream/StreamRecordReader.java b/cdap-data-fabric/src/main/java/co/cask/cdap/data/stream/StreamRecordReader.java
index db7de11..514e798 100644
--- a/cdap-data-fabric/src/main/java/co/cask/cdap/data/stream/StreamRecordReader.java
+++ b/cdap-data-fabric/src/main/java/co/cask/cdap/data/stream/StreamRecordReader.java
@@ -68 +68 @@
-      if (reader.getPosition() >= inputSplit.getStart() + inputSplit.getLength()) {
+      if (reader.getPosition() - inputSplit.getStart() >= inputSplit.getLength()) {


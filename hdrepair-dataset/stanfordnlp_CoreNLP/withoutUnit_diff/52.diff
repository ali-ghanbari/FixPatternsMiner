diff --git a/doc/segmenter/README-Arabic.txt b/doc/segmenter/README-Arabic.txt
index 9c16776..8fce4fb 100644
--- a/doc/segmenter/README-Arabic.txt
+++ b/doc/segmenter/README-Arabic.txt
@@ -1 +1 @@
-Stanford Arabic Segmenter - v3.3.1 - 2014-01-04
+Stanford Arabic Segmenter - v3.4.1 <<CHECK>> - 2014-05-03
@@ -82,0 +83,18 @@
+SEGMENTING DIALECTAL TEXT
+
+As of version 3.4.1 <<CHECK>>, the segmenter supports Egyptian dialect using
+domain adaptation. [Hal Daum√© III, Frustratingly Easy Domain Adaptation, ACL
+2007] To indicate that the provided text is in Egyptian dialect, add the
+command-line option:
+
+  -domain arz
+
+You can also construct a file that specifies a dialect for each
+newline-separated sentence, by adding "atb" or "arz" at the beginning of each
+line followed by a tab character. This feature is enabled with the flag:
+
+  -withDomains
+
+See the bottom of the next section for information about training the
+segmenter on your own dialectal data.
+
@@ -109,0 +128,9 @@
+Adding the -withDomains flag lets you specify a domain (offset by a tab) at
+the beginning of each sentence in the training file. These domains can be
+arbitrary strings, as long as they don't contain tab or newline characters;
+thus, if you have data available for other dialects in ATB format, it is
+possible to train your own system that can support these dialects. For best
+results, include MSA data as well as your dialect data in your training.
+(Adding data from dialects other than your target dialect should not hurt
+performance, as long as they are marked as different domains--it may even
+help!)
diff --git a/src/edu/stanford/nlp/international/arabic/process/ArabicSegmenter.java b/src/edu/stanford/nlp/international/arabic/process/ArabicSegmenter.java
index 341acbe..57e3268 100644
--- a/src/edu/stanford/nlp/international/arabic/process/ArabicSegmenter.java
+++ b/src/edu/stanford/nlp/international/arabic/process/ArabicSegmenter.java
@@ -87 +87 @@
-  // Ignore rewrites (training only, produces a model than then can be used to do
+  // Ignore rewrites (training only, produces a model that then can be used to do


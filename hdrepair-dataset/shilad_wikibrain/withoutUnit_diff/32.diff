diff --git a/wikAPIdia-sr/src/main/java/org/wikapidia/sr/ESAMetric.java b/wikAPIdia-sr/src/main/java/org/wikapidia/sr/ESAMetric.java
index fa8cc44..cef4170 100644
--- a/wikAPIdia-sr/src/main/java/org/wikapidia/sr/ESAMetric.java
+++ b/wikAPIdia-sr/src/main/java/org/wikapidia/sr/ESAMetric.java
@@ -115,0 +116 @@
+     *
diff --git a/wikAPIdia-sr/src/main/python/combine_gold.py b/wikAPIdia-sr/src/main/python/combine_gold.py
new file mode 100644
index 0000000..4baec8a
--- /dev/null
+++ b/wikAPIdia-sr/src/main/python/combine_gold.py
@@ -0,0 +1,64 @@
+#!/usr/bin/python -O
+
+import collections
+import os
+import sys
+
+MIN_SIM = 0
+MAX_SIM = 1.0
+
+def main(paths):
+    scores = collections.defaultdict(list)
+    for path in sys.argv[1:]:
+        for (w1, w2, score) in getSims(path):
+            scores[(w1, w2)].append(score)
+
+    num_duplicate = 0
+    for ((w1, w2), values) in scores.items():
+        if len(values) > 1:
+            #warn('pair (%s, %s) has scores: %s' % (w1, w2, values))
+            num_duplicate += 1
+        print '%s\t%s\t%s' % (w1, w2, mean(values))
+
+def clean(s):
+    return s.replace('_', ' ').lower()
+
+def getSims(path):
+    delim = None
+    if path.endswith('csv'):
+        delim = ','
+    elif path.endswith('tab'):
+        delim = '\t'
+    else:
+        raise Exception('unknown delimiter for file ' + path)
+
+    name = os.path.basename(path).split('.')[0]
+    sims = []
+    for line in open(path):
+        try:
+            (w1, w2, s) = line.strip().split(delim)
+            sims.append((w1, w2, float(s)))
+        except:
+            warn('invalid line in %s: %s' % (`path`, `line`))
+
+    min_sim = min([s for w1, w2, s in sims])
+    max_sim = max([s for w1, w2, s in sims])
+    warn('%s has sims in range [%s to %s]' % (`path`, min_sim, max_sim))
+
+    result = []
+    for (w1, w2, s) in sims:
+        if w1 > w2: t = w1; w1 = w2; w2 = t
+        norm_s = (s - min_sim) / (max_sim - min_sim)
+        new_s = norm_s * (MAX_SIM - MIN_SIM) + MIN_SIM
+        result.append((clean(w1), clean(w2), new_s))
+
+    return result
+
+def mean(values):
+    return 1.0 * sum(values) / len(values)
+
+def warn(message):
+    sys.stderr.write(message + '\n')
+
+if __name__ == '__main__':
+    main(sys.argv[1:])
diff --git a/wikAPIdia-sr/src/main/python/filter_gold.py b/wikAPIdia-sr/src/main/python/filter_gold.py
new file mode 100644
index 0000000..8c8b52d
--- /dev/null
+++ b/wikAPIdia-sr/src/main/python/filter_gold.py
@@ -0,0 +1,39 @@
+#!/usr/bin/python
+
+import collections
+import sys
+from math import floor
+from random import sample
+
+NBINS = int(sys.argv[1])
+
+min_threshold = -10000000000
+if len(sys.argv) >= 3:
+    min_threshold = float(sys.argv[2])
+
+
+data = []
+for line in sys.stdin:
+    (phrase1, phrase2, sim) = line.split('\t')
+    sim = float(sim.strip())
+    if sim >= min_threshold:
+        data.append((phrase1.strip(), phrase2.strip(), sim, line))
+
+min_sim = min([s for (p1, p2, s, l) in data])
+max_sim = max([s for (p1, p2, s, l) in data])
+
+bins = collections.defaultdict(list)
+for (p1, p2, s, l) in data:
+    i = int(floor((s - min_sim) / (.0001 + max_sim - min_sim) * NBINS))
+    bins[i].append((p1, p2, s, l))
+
+for i in xrange(NBINS):
+    sys.stderr.write('bin %d has %d entries\n' % (i, len(bins[i])))
+min_bin_size = min([len(b) for b in bins.values()])
+sys.stderr.write('min bin size is %d\n' % min_bin_size)
+
+for bin_data in bins.values():
+    if len(bin_data) > min_bin_size:
+        bin_data = sample(bin_data, min_bin_size)
+    for (p1, p2, s, l) in bin_data:
+        sys.stdout.write(l)
diff --git a/wikAPIdia-sr/src/main/scripts/make-gold.sh b/wikAPIdia-sr/src/main/scripts/make-gold.sh
new file mode 100755
index 0000000..dc31192
--- /dev/null
+++ b/wikAPIdia-sr/src/main/scripts/make-gold.sh
@@ -0,0 +1,83 @@
+#!/bin/bash
+
+DL=../dat/gold/dl
+SRC=../dat/gold/src  # src datasets
+CLEANED=../dat/gold/cleaned  # cleaned source datasets
+
+
+rm -rf $DL
+mkdir -p $DL
+rm -rf $SRC
+mkdir -p $SRC
+rm -rf $CLEANED
+mkdir -p $CLEANED
+
+mkdir $SRC/titles   # wikipedia titles to similarity scores
+mkdir $SRC/phrases  # phrases similarity scores
+
+# Downloads datasets and combines them into a single gold standard
+
+
+# Gabrilovich et al, 2002
+# see http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/
+wget -P $DL http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/wordsim353.zip &&
+mkdir $DL/wordsim353 &&
+unzip -d $DL/wordsim353 $DL/wordsim353.zip &&
+tail -n '+2' $DL/wordsim353/combined.csv > $SRC/phrases/wordsim353.csv || 
+{ echo "ERROR: preparing wordsim353 failed" >&2; exit 1;}
+
+# A version of wordsim353 manually matched to wikipedia articles
+wget -P $DL http://www.nzdl.org/wikipediaSimilarity/wikipediaSimilarity353.csv &&
+mkdir $DL/wikisim353 &&
+tail -n '+2' $DL/wikipediaSimilarity353.csv |
+cut -f 3,6,7 -d ',' |
+tr -d '\r' >$SRC/titles/wikisim353.csv ||
+{ echo "ERROR: preparing wikisim353 dataset failed" >&2; exit 1;}
+
+# MTurk, Radinsky et al, 2011
+# see http://www.technion.ac.il/~kirar/Datasets.html
+wget -P $DL http://www.technion.ac.il/~kirar/files/Mtruk.csv &&
+cp -p $DL/Mtruk.csv $SRC/phrases/radinsky.csv || 
+{ echo "ERROR: preparing radinsky dataset failed" >&2; exit 1;}
+
+# Concept sim, Miller et al, 1991
+# http://www.seas.upenn.edu/~hansens/conceptSim/
+wget -P $DL http://www.seas.upenn.edu/~hansens/conceptSim/ConceptSim.tar.gz &&
+tar -C $DL -xzvf $DL/ConceptSim.tar.gz &&
+sed -e 's/	[	]*/,/g' < $DL/ConceptSim/MC_word.txt  > $SRC/phrases/MC.csv &&
+sed -e 's/	[	]*/,/g' < $DL/ConceptSim/RG_word.txt  > $SRC/phrases/RG.csv ||
+{ echo "ERROR: preparing conceptsim dataset failed" >&2; exit 1;}
+
+# Atlasify: Hecht et al, 2012
+#
+wget -P $DL http://www.cs.northwestern.edu/~ddowney/data/atlasify240.csv &&
+cp -p $DL/atlasify240.csv $SRC/phrases/atlasify240.csv ||
+{ echo "ERROR: preparing atlasify dataset failed" >&2; exit 1;}
+
+# WikiSimi
+#
+wget -P $DL http://sigwp.org/wikisimi/WikiSimi3000_1.csv &&
+cp -p $DL/WikiSimi3000_1.csv  $SRC/titles/WikiSimi3000.tab ||
+{ echo "ERROR: preparing wikisimi dataset failed" >&2; exit 1;}
+
+
+for d in titles phrases; do
+    python src/main/python/combine_gold.py $SRC/$d/*.* |
+    python src/main/python/filter_gold.py 10 >dat/gold/gold.$d.similarity.txt ||
+        { echo "ERROR: combining $d datasets failed" >&2; exit 1;}
+
+    python src/main/python/combine_gold.py $SRC/$d/*.* |
+    python src/main/python/filter_gold.py 4 0.6 >dat/gold/gold.$d.mostSimilar.txt ||
+        { echo "ERROR: combining $d datasets failed" >&2; exit 1;}
+done
+
+# Create individual datasets
+for d in $SRC/phrases $SRC/titles; do
+    for file in `ls $d`; do
+        txt=`echo $file | sed 's/\..*$/.txt/'`
+        python src/main/python/combine_gold.py $d/$file >$CLEANED/$txt
+    done
+done
+
+
+echo "SUCCESS!"

